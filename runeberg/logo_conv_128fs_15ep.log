 python classifier.py conv
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
time: 2017-12-13 11:18:40.097808, epochs: 15, learning rate: 0.0001, training size: 100000, test size: 94875
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 293)               0         
_________________________________________________________________
emb_layer (Embedding)        (None, 293, 100)          2017200   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 293, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 146, 128)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 18688)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 13)                242957    
=================================================================
Total params: 2,324,285
Trainable params: 2,324,285
Non-trainable params: 0
_________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/15
83200/83200 [==============================] - 554s - loss: 2.0434 - acc: 0.2167 - val_loss: 1.7078 - val_acc: 0.2886
Epoch 2/15
83200/83200 [==============================] - 582s - loss: 1.6110 - acc: 0.3325 - val_loss: 1.5792 - val_acc: 0.3452
Epoch 3/15
83200/83200 [==============================] - 623s - loss: 1.4731 - acc: 0.3896 - val_loss: 1.5091 - val_acc: 0.3731
Epoch 4/15
83200/83200 [==============================] - 658s - loss: 1.3772 - acc: 0.4253 - val_loss: 1.4686 - val_acc: 0.3947
Epoch 5/15
83200/83200 [==============================] - 680s - loss: 1.3022 - acc: 0.4567 - val_loss: 1.4466 - val_acc: 0.4026
Epoch 6/15
83200/83200 [==============================] - 676s - loss: 1.2417 - acc: 0.4823 - val_loss: 1.4343 - val_acc: 0.4143
Epoch 7/15
83200/83200 [==============================] - 678s - loss: 1.1894 - acc: 0.5078 - val_loss: 1.4361 - val_acc: 0.4169
Epoch 8/15
83200/83200 [==============================] - 672s - loss: 1.1428 - acc: 0.5284 - val_loss: 1.4369 - val_acc: 0.4238
Epoch 9/15
83200/83200 [==============================] - 551s - loss: 1.0990 - acc: 0.5502 - val_loss: 1.4416 - val_acc: 0.4303
Epoch 10/15
83200/83200 [==============================] - 448s - loss: 1.0588 - acc: 0.5679 - val_loss: 1.4463 - val_acc: 0.4354
Epoch 11/15
83200/83200 [==============================] - 492s - loss: 1.0203 - acc: 0.5855 - val_loss: 1.4654 - val_acc: 0.4368
Epoch 12/15
83200/83200 [==============================] - 490s - loss: 0.9844 - acc: 0.6049 - val_loss: 1.4818 - val_acc: 0.4388
Epoch 13/15
83200/83200 [==============================] - 483s - loss: 0.9496 - acc: 0.6230 - val_loss: 1.5014 - val_acc: 0.4421
Epoch 14/15
83200/83200 [==============================] - 497s - loss: 0.9164 - acc: 0.6383 - val_loss: 1.5231 - val_acc: 0.4438
Epoch 15/15
83200/83200 [==============================] - 562s - loss: 0.8846 - acc: 0.6531 - val_loss: 1.5528 - val_acc: 0.4451
94875/94875 [==============================] - 231s     
1.59653722261 0.416769433466
[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]
[11  3  3  2  3  3  5  0  3  3  7  5  4  3  5  3  4  2  3  3  2  2  2  4  4
  5  5  4  5  3  4  3  4  6  2  3  2  0  4  2  5  2  2  3  5  4  5  0  4  3
  4  4  3  3  3  3  4  6  3  6  4  5  5  6  3  3  2  5  2  4  3  1  3  3  5
  2  0  7  3  2  3  5  3  3  3  4  2  3  3  2  4  4  4  5  5  3  1  2  0  2]
[[2168  895  314   71   50   26    3   14   17    1    1   10    0]
 [1785 3541 1993  264  125  153   35   34   28    7   12   20    3]
 [ 638 1525 3455 1318  269  487  142   59   48   11   17   23    8]
 [ 270  203 1450 3231 1549  742  276  150   48   12   10   54    5]
 [ 109   62  367 1842 3187 1306  610  317   70   22   23   73   12]
 [ 120  111  580  838 1318 2492 1609  626  121   40   75   47   23]
 [  50   33  227  260  722 1490 2798 1741  296   63  180   96   44]
 [  51   16  121  118  384  553 1856 2965 1026  221  406  225   58]
 [  41   13   62   49  101  117  281 1103 3290 1847  757  283   56]
 [   4    1   11    6   11    3   23   97  609 1965  353  185   37]
 [   3    3   27   10   17   38  136  224  482  819 3362 1945  934]
 [   1    1   20    7   32   24   88  154  207  517 2362 2582 2005]
 [   2    1   11    1   13   18   45   69   76  224 1164 1871 4505]]
             precision    recall  f1-score   support

          0       0.41      0.61      0.49      3570
          1       0.55      0.44      0.49      8000
          2       0.40      0.43      0.42      8000
          3       0.40      0.40      0.40      8000
          4       0.41      0.40      0.40      8000
          5       0.33      0.31      0.32      8000
          6       0.35      0.35      0.35      8000
          7       0.39      0.37      0.38      8000
          8       0.52      0.41      0.46      8000
          9       0.34      0.59      0.43      3305
         10       0.39      0.42      0.40      8000
         11       0.35      0.32      0.34      8000
         12       0.59      0.56      0.57      8000

avg / total       0.42      0.42      0.42     94875

