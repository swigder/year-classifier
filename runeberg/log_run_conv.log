python classifier.py conv
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
embedding model not found
time: 2017-12-12 12:38:19.562209, epochs: 15, learning rate: 0.0001, training size: 100000, test size: 94875
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 293)               0         
_________________________________________________________________
emb_layer (Embedding)        (None, 293, 100)          2017200   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 293, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 146, 128)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 18688)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 13)                242957    
=================================================================
Total params: 2,324,285
Trainable params: 2,324,285
Non-trainable params: 0
_________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/15
83200/83200 [==============================] - 343s - loss: 2.1632 - acc: 0.1979 - val_loss: 1.8958 - val_acc: 0.2538
Epoch 2/15
83200/83200 [==============================] - 376s - loss: 1.8135 - acc: 0.2829 - val_loss: 1.7793 - val_acc: 0.2871
Epoch 3/15
83200/83200 [==============================] - 607s - loss: 1.6985 - acc: 0.3276 - val_loss: 1.7003 - val_acc: 0.3247
Epoch 4/15
83200/83200 [==============================] - 591s - loss: 1.6019 - acc: 0.3634 - val_loss: 1.6405 - val_acc: 0.3388
Epoch 5/15
83200/83200 [==============================] - 588s - loss: 1.5284 - acc: 0.3885 - val_loss: 1.5971 - val_acc: 0.3586
Epoch 6/15
83200/83200 [==============================] - 602s - loss: 1.4716 - acc: 0.4125 - val_loss: 1.5729 - val_acc: 0.3625
Epoch 7/15
83200/83200 [==============================] - 597s - loss: 1.4244 - acc: 0.4297 - val_loss: 1.5512 - val_acc: 0.3796
Epoch 8/15
83200/83200 [==============================] - 627s - loss: 1.3832 - acc: 0.4473 - val_loss: 1.5553 - val_acc: 0.3759
Epoch 9/15
83200/83200 [==============================] - 696s - loss: 1.3454 - acc: 0.4669 - val_loss: 1.5317 - val_acc: 0.3926
Epoch 10/15
83200/83200 [==============================] - 695s - loss: 1.3110 - acc: 0.4809 - val_loss: 1.5224 - val_acc: 0.4027
Epoch 11/15
83200/83200 [==============================] - 703s - loss: 1.2766 - acc: 0.4981 - val_loss: 1.5276 - val_acc: 0.4066
Epoch 12/15
83200/83200 [==============================] - 715s - loss: 1.2468 - acc: 0.5155 - val_loss: 1.5252 - val_acc: 0.4095
Epoch 13/15
83200/83200 [==============================] - 708s - loss: 1.2163 - acc: 0.5296 - val_loss: 1.5203 - val_acc: 0.4149
Epoch 14/15
83200/83200 [==============================] - 687s - loss: 1.1871 - acc: 0.5442 - val_loss: 1.5269 - val_acc: 0.4167
Epoch 15/15
83200/83200 [==============================] - 715s - loss: 1.1595 - acc: 0.5554 - val_loss: 1.5221 - val_acc: 0.4221
94875/94875 [==============================] - 290s     
5.05266763115 0.0931963109358
[12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12
 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12
 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12
 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12]
[ 3  6  0  7  8  7  8  4  3  7  4  4  7  4  8  8  7  5 10  6  8  5  8  7  9
 11  6  8  5  5  7  7  0  8  8  7 12  5  7 10  4 11  8  0  8  7  8  8  5  5
  4  7  5 12 12  2  7  7  8  7  8  8 12  6  5 12  7  7  4  8  8  7  6  7  8
  8  8  6 12  0  7 10  8  7  3  7  7  8  7  4  8  7  8  7  6  6  3  4  4  8]
[[ 132  136  196  207  460  332  228  409 1190   36  112   60   72]
 [ 239  299  428  486 1143  693  495  909 2821   75  203   89  120]
 [ 237  264  376  407 1085  736  546 1130 2639   70  234  123  153]
 [ 172  184  306  486  942  548  685 1394 2587   72  237  140  247]
 [ 143  179  262  430 1007  560  731 1420 2638   89  244   99  198]
 [ 171  167  294  347  917  666  775 1420 2543   72  264  128  236]
 [ 139  160  285  286  948  645  833 1477 2550   63  295  127  192]
 [ 142  150  311  342 1051  683  750 1368 2602   63  229  129  180]
 [ 140  137  264  359 1087  561  595 1376 2862   94  219   99  207]
 [  29   25   68  117  438  230  246  634 1252   41   96   45   84]
 [ 207  237  330  338  926  631  701 1364 2588   54  292  106  226]
 [ 203  199  320  329  897  634  662 1360 2708   72  259  123  234]
 [ 184  182  287  281  780  633  661 1514 2528   86  346  161  357]]
             precision    recall  f1-score   support

          0       0.06      0.04      0.05      3570
          1       0.13      0.04      0.06      8000
          2       0.10      0.05      0.06      8000
          3       0.11      0.06      0.08      8000
          4       0.09      0.13      0.10      8000
          5       0.09      0.08      0.09      8000
          6       0.11      0.10      0.10      8000
          7       0.09      0.17      0.12      8000
          8       0.09      0.36      0.14      8000
          9       0.05      0.01      0.02      3305
         10       0.10      0.04      0.05      8000
         11       0.09      0.02      0.03      8000
         12       0.14      0.04      0.07      8000

avg / total       0.10      0.09      0.08     94875

