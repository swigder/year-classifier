python classifier.py conv
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
time: 2017-12-14 09:56:58.695619, epochs: 15, learning rate: 3e-05, training size: 100000, test size: 94875
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 293)               0         
_________________________________________________________________
emb_layer (Embedding)        (None, 293, 100)          2017200   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 293, 32)           9632      
_________________________________________________________________
batch_normalization_1 (Batch (None, 293, 32)           128       
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 146, 32)           0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4672)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 13)                60749     
=================================================================
Total params: 2,087,709
Trainable params: 2,087,645
Non-trainable params: 64
_________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/15
83200/83200 [==============================] - 154s - loss: 2.4778 - acc: 0.1290 - val_loss: 2.2890 - val_acc: 0.1619
Epoch 2/15
83200/83200 [==============================] - 149s - loss: 1.9654 - acc: 0.2521 - val_loss: 1.8120 - val_acc: 0.2855
Epoch 3/15
83200/83200 [==============================] - 133s - loss: 1.7001 - acc: 0.3266 - val_loss: 1.6505 - val_acc: 0.3259
Epoch 4/15
83200/83200 [==============================] - 131s - loss: 1.5646 - acc: 0.3680 - val_loss: 1.5690 - val_acc: 0.3554
Epoch 5/15
83200/83200 [==============================] - 150s - loss: 1.4824 - acc: 0.3955 - val_loss: 1.5230 - val_acc: 0.3749
Epoch 6/15
83200/83200 [==============================] - 160s - loss: 1.4209 - acc: 0.4196 - val_loss: 1.4890 - val_acc: 0.3870
Epoch 7/15
83200/83200 [==============================] - 161s - loss: 1.3693 - acc: 0.4435 - val_loss: 1.4648 - val_acc: 0.3930
Epoch 8/15
83200/83200 [==============================] - 160s - loss: 1.3224 - acc: 0.4626 - val_loss: 1.4473 - val_acc: 0.3999
Epoch 9/15
83200/83200 [==============================] - 162s - loss: 1.2800 - acc: 0.4819 - val_loss: 1.4318 - val_acc: 0.4085
Epoch 10/15
83200/83200 [==============================] - 162s - loss: 1.2398 - acc: 0.5001 - val_loss: 1.4233 - val_acc: 0.4169
Epoch 11/15
83200/83200 [==============================] - 162s - loss: 1.2003 - acc: 0.5189 - val_loss: 1.4144 - val_acc: 0.4207
Epoch 12/15
83200/83200 [==============================] - 161s - loss: 1.1625 - acc: 0.5357 - val_loss: 1.4061 - val_acc: 0.4283
Epoch 13/15
83200/83200 [==============================] - 161s - loss: 1.1253 - acc: 0.5538 - val_loss: 1.4013 - val_acc: 0.4320
Epoch 14/15
83200/83200 [==============================] - 162s - loss: 1.0897 - acc: 0.5722 - val_loss: 1.3982 - val_acc: 0.4387
Epoch 15/15
83200/83200 [==============================] - 146s - loss: 1.0543 - acc: 0.5899 - val_loss: 1.3992 - val_acc: 0.4425
94875/94875 [==============================] - 54s     
1.44800955283 0.411857707513
[11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]
[10 12 11 10 11 10 11 11 12 11 11 10 10 11 11 10 11 11 11 10 11 10 11 11 11
  9 12 12 10 11 12 11  9 12 12  9 12 10 11 11 10 12 11  5  6 11 11  9  7 10
 12 10 11  8  3 11 11  8 10 11 11 11 10 11 11 11 10 11 10  8 11 12  9 12 12
 10 12 10 10 11 11 11 11 10 12  7 12 12 10 11 11  2 11 11 11  9 11 11 11 12]
[[2394  635  382   90   25   19    1   10    4    0    0   10    0]
 [2356 3085 1977  298   70  113   12   43   17    3    3   20    3]
 [ 913 1385 3216 1471  270  484   98   90   25    5    4   33    6]
 [ 264  129 1568 3044 1655  783  275  183   31    8    3   51    6]
 [  97   33  441 1834 3128 1248  649  421   52   15    9   65    8]
 [ 119   51  611  981 1257 2577 1361  806  115   17   22   66   17]
 [  39   20  194  310  734 1678 2410 2017  253   71  100  135   39]
 [  38   19  126  165  369  662 1651 3184  865  214  302  352   53]
 [  21   10   63   66   97  119  198 1193 3197 2046  538  394   58]
 [   3    0    7    8    3    5   16  113  563 2086  262  215   24]
 [   6    3   19    9    3   14  102  352  395  802 3025 2385  885]
 [   8    1   18   10    6   23   62  263  168  443 1884 3149 1965]
 [   5    3    7    4    1    8   44  121   64  165  881 2117 4580]]
             precision    recall  f1-score   support

          0       0.38      0.67      0.49      3570
          1       0.57      0.39      0.46      8000
          2       0.37      0.40      0.39      8000
          3       0.37      0.38      0.37      8000
          4       0.41      0.39      0.40      8000
          5       0.33      0.32      0.33      8000
          6       0.35      0.30      0.32      8000
          7       0.36      0.40      0.38      8000
          8       0.56      0.40      0.47      8000
          9       0.36      0.63      0.45      3305
         10       0.43      0.38      0.40      8000
         11       0.35      0.39      0.37      8000
         12       0.60      0.57      0.59      8000

avg / total       0.42      0.41      0.41     94875

