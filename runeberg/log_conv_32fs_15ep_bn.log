 python classifier.py conv
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
time: 2017-12-14 09:21:47.241892, epochs: 15, learning rate: 0.0003, training size: 100000, test size: 94875
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 293)               0         
_________________________________________________________________
emb_layer (Embedding)        (None, 293, 100)          2017200   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 293, 32)           9632      
_________________________________________________________________
batch_normalization_1 (Batch (None, 293, 32)           128       
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 146, 32)           0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4672)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 13)                60749     
=================================================================
Total params: 2,087,709
Trainable params: 2,087,645
Non-trainable params: 64
_________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/15
83200/83200 [==============================] - 132s - loss: 1.7024 - acc: 0.3225 - val_loss: 1.4503 - val_acc: 0.3908
Epoch 2/15
83200/83200 [==============================] - 156s - loss: 1.2333 - acc: 0.5017 - val_loss: 1.4905 - val_acc: 0.4079
Epoch 3/15
83200/83200 [==============================] - 160s - loss: 0.9811 - acc: 0.6167 - val_loss: 1.5610 - val_acc: 0.4258
Epoch 4/15
83200/83200 [==============================] - 159s - loss: 0.7427 - acc: 0.7274 - val_loss: 1.8814 - val_acc: 0.4034
Epoch 5/15
83200/83200 [==============================] - 160s - loss: 0.5006 - acc: 0.8329 - val_loss: 2.0528 - val_acc: 0.4137
Epoch 6/15
83200/83200 [==============================] - 160s - loss: 0.3003 - acc: 0.9101 - val_loss: 2.4957 - val_acc: 0.4156
Epoch 7/15
83200/83200 [==============================] - 161s - loss: 0.1675 - acc: 0.9550 - val_loss: 2.7152 - val_acc: 0.4253
Epoch 8/15
83200/83200 [==============================] - 161s - loss: 0.0963 - acc: 0.9760 - val_loss: 3.4349 - val_acc: 0.4170
Epoch 9/15
83200/83200 [==============================] - 160s - loss: 0.0606 - acc: 0.9856 - val_loss: 3.8128 - val_acc: 0.4028
Epoch 10/15
83200/83200 [==============================] - 160s - loss: 0.0434 - acc: 0.9892 - val_loss: 3.8367 - val_acc: 0.4149
Epoch 11/15
83200/83200 [==============================] - 158s - loss: 0.0346 - acc: 0.9916 - val_loss: 4.4002 - val_acc: 0.3933
Epoch 12/15
83200/83200 [==============================] - 137s - loss: 0.0323 - acc: 0.9916 - val_loss: 4.6682 - val_acc: 0.3900
Epoch 13/15
83200/83200 [==============================] - 135s - loss: 0.0292 - acc: 0.9922 - val_loss: 4.6362 - val_acc: 0.4091
Epoch 14/15
83200/83200 [==============================] - 140s - loss: 0.0233 - acc: 0.9939 - val_loss: 4.8073 - val_acc: 0.3997
Epoch 15/15
83200/83200 [==============================] - 160s - loss: 0.0218 - acc: 0.9941 - val_loss: 5.1486 - val_acc: 0.3950
94848/94875 [============================>.] - ETA: 0s5.21106684137 0.37480895916
[11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]
[11 12 12 11  7 10 12 12 12 12 11  6 12 11 11 11 12 11 12 11 12 11 10  7 11
 12 12 11  7 10 11 10 10 10 11 11 12 10 11 12 10 12 11 12 12 11 12 11  7  8
 11 11 10  7  5 12  4 11 11 11 11 12 12 12 10 11  8 11 10 12 12 11 10 11 12
 11 12  5 12 12 12 11 12 11 12 12 12 12 12 11 12  1 12 10 12  6 12 12  9 11]
[[1846 1075  189  185   42  167   21   30    3    1    0    0   11]
 [1403 4120 1024  547  147  536  112   67    7    0    1    9   27]
 [ 492 2101 1908 1528  358 1074  300  174   17    1    2   11   34]
 [ 158  518  653 3143 1244 1394  502  300   20    7    3    8   50]
 [  59  176  187 1748 2386 1948  864  495   28   14    7   21   67]
 [  67  203  213  812  924 3263 1595  704   68   19   12   46   74]
 [  19   88   89  348  517 2074 2625 1769  147   48   41   88  147]
 [  12   68   63  234  292 1072 1966 2998  554  156  120  190  275]
 [  14   33   32   89   74  453  650 1580 2438 1421  303  498  415]
 [   0    0    1    9   11  102   96  250  576 1572  164  282  242]
 [   3   18   11   24   18  165  303  667  406  532 1501 2190 2162]
 [   1    8    8   20   20  137  241  419  229  268  879 2416 3354]
 [   0    2    4   18   10   73  117  191   76  129  452 1584 5344]]
             precision    recall  f1-score   support

          0       0.45      0.52      0.48      3570
          1       0.49      0.52      0.50      8000
          2       0.44      0.24      0.31      8000
          3       0.36      0.39      0.38      8000
          4       0.39      0.30      0.34      8000
          5       0.26      0.41      0.32      8000
          6       0.28      0.33      0.30      8000
          7       0.31      0.37      0.34      8000
          8       0.53      0.30      0.39      8000
          9       0.38      0.48      0.42      3305
         10       0.43      0.19      0.26      8000
         11       0.33      0.30      0.31      8000
         12       0.44      0.67      0.53      8000

avg / total       0.39      0.37      0.37     94875


