python classifier.py conv
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
time: 2017-12-14 10:07:22.919241, epochs: 15, learning rate: 3e-05, training size: 100000, test size: 94875
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 293)               0         
_________________________________________________________________
emb_layer (Embedding)        (None, 293, 100)          2017200   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 293, 32)           9632      
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 146, 32)           0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4672)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 13)                60749     
=================================================================
Total params: 2,087,581
Trainable params: 2,087,581
Non-trainable params: 0
_________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/15
83200/83200 [==============================] - 102s - loss: 2.5500 - acc: 0.1034 - val_loss: 2.4929 - val_acc: 0.1407
Epoch 2/15
83200/83200 [==============================] - 106s - loss: 2.3040 - acc: 0.1827 - val_loss: 2.1527 - val_acc: 0.2009
Epoch 3/15
83200/83200 [==============================] - 106s - loss: 2.0713 - acc: 0.2171 - val_loss: 2.0134 - val_acc: 0.2205
Epoch 4/15
83200/83200 [==============================] - 106s - loss: 1.9607 - acc: 0.2381 - val_loss: 1.9353 - val_acc: 0.2412
Epoch 5/15
83200/83200 [==============================] - 106s - loss: 1.8924 - acc: 0.2500 - val_loss: 1.8856 - val_acc: 0.2453
Epoch 6/15
83200/83200 [==============================] - 106s - loss: 1.8427 - acc: 0.2608 - val_loss: 1.8486 - val_acc: 0.2564
Epoch 7/15
83200/83200 [==============================] - 107s - loss: 1.8038 - acc: 0.2712 - val_loss: 1.8226 - val_acc: 0.2623
Epoch 8/15
83200/83200 [==============================] - 107s - loss: 1.7714 - acc: 0.2823 - val_loss: 1.7971 - val_acc: 0.2695
Epoch 9/15
83200/83200 [==============================] - 106s - loss: 1.7434 - acc: 0.2909 - val_loss: 1.7770 - val_acc: 0.2758
Epoch 10/15
83200/83200 [==============================] - 106s - loss: 1.7188 - acc: 0.2989 - val_loss: 1.7608 - val_acc: 0.2763
Epoch 11/15
83200/83200 [==============================] - 107s - loss: 1.6960 - acc: 0.3062 - val_loss: 1.7466 - val_acc: 0.2799
Epoch 12/15
83200/83200 [==============================] - 107s - loss: 1.6746 - acc: 0.3130 - val_loss: 1.7319 - val_acc: 0.2904
Epoch 13/15
83200/83200 [==============================] - 106s - loss: 1.6548 - acc: 0.3202 - val_loss: 1.7216 - val_acc: 0.2911
Epoch 14/15
83200/83200 [==============================] - 108s - loss: 1.6356 - acc: 0.3276 - val_loss: 1.7076 - val_acc: 0.2985
Epoch 15/15
83200/83200 [==============================] - 105s - loss: 1.6162 - acc: 0.3365 - val_loss: 1.6924 - val_acc: 0.3106
94848/94875 [============================>.] - ETA: 0s1.71439055423 0.299942028986
[11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]
[10 11  9 11 10 11 11 11 12 12  8 10 12 12  8 12 10 10 10 10  8 12 10 11 11
 10 12 11  8 12 12 11  9 11 10  9 12 12 11 12 10  8  9  7  9  8 12 10 10  9
 12 11 11 12  3 10 11  9  9  9 11  8 11 11 10 12  9 12 10  8 12 12 11 10  8
 10 12 10 12 12  8 11  8 12 12 10 12 12  9 10 11  7 10 12  3 10 12 10 12 12]
[[1731  938  707  127    4   54    1    6    2    0    0    0    0]
 [2875 2499 1979  329   29  229   19   30   11    0    0    0    0]
 [1374 1249 3181 1005  190  709  174   86   28    2    0    1    1]
 [ 330  227 2435 1912  970 1242  593  222   66    0    1    2    0]
 [  92   53 1119 1957 1371 1545 1329  391  112   13   15    3    0]
 [  87   56 1187 1136  602 1854 1979  780  253   26   24   12    4]
 [  18   14  238  561  424 1245 3004 1603  631   88  138   26   10]
 [  13    7  117  375  142  769 2380 2055 1346  306  325  124   41]
 [   3    6   46  111   24  259  493 1149 2508 1611  872  608  310]
 [   0    0    0   15    0   11   44  172  753  908  574  507  321]
 [   1    0    0   37    3   25   85  379 1407 1321 2092 1351 1299]
 [   1    0    2   83    5   13   47  225 1080 1023 1769 1706 2046]
 [   0    0    0   46    5   10   18  114  682  624 1361 1504 3636]]
             precision    recall  f1-score   support

          0       0.27      0.48      0.34      3570
          1       0.49      0.31      0.38      8000
          2       0.29      0.40      0.33      8000
          3       0.25      0.24      0.24      8000
          4       0.36      0.17      0.23      8000
          5       0.23      0.23      0.23      8000
          6       0.30      0.38      0.33      8000
          7       0.28      0.26      0.27      8000
          8       0.28      0.31      0.30      8000
          9       0.15      0.27      0.20      3305
         10       0.29      0.26      0.28      8000
         11       0.29      0.21      0.25      8000
         12       0.47      0.45      0.46      8000

avg / total       0.31      0.30      0.30     94875

