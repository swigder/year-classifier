average

python conv_network.py 
Using TensorFlow backend.
['1856', '1865', '1875', '1883', '1897', '1903', '1911', '1924', '1936', '1945', '1957', '1965', '1972', '1983']
time: 2017-12-07 11:28:02.943995, epochs: 10, learning rate: 0.001, training size: 10000, test size: 2000
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
emb_layer (Embedding)        (None, 447, 300)          32738100  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 445, 128)          115328    
_________________________________________________________________
average_pooling1d_1 (Average (None, 222, 128)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 28416)             0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 28416)             113664    
_________________________________________________________________
dropout_1 (Dropout)          (None, 28416)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 14)                397838    
=================================================================
Total params: 33,364,930
Trainable params: 569,998
Non-trainable params: 32,794,932
_________________________________________________________________
None
Train on 8000 samples, validate on 2000 samples
Epoch 1/10
8000/8000 [==============================] - 134s - loss: 2.4039 - acc: 0.2712 - val_loss: 2.6134 - val_acc: 0.1795
Epoch 2/10
8000/8000 [==============================] - 127s - loss: 2.3169 - acc: 0.3575 - val_loss: 2.5654 - val_acc: 0.3235
Epoch 3/10
8000/8000 [==============================] - 127s - loss: 2.2722 - acc: 0.4057 - val_loss: 2.4128 - val_acc: 0.3615
Epoch 4/10
8000/8000 [==============================] - 127s - loss: 2.2234 - acc: 0.4341 - val_loss: 2.5520 - val_acc: 0.3400
Epoch 5/10
8000/8000 [==============================] - 125s - loss: 2.1861 - acc: 0.4583 - val_loss: 2.6606 - val_acc: 0.3270
Epoch 6/10
8000/8000 [==============================] - 127s - loss: 2.1784 - acc: 0.4649 - val_loss: 2.5796 - val_acc: 0.3530
Epoch 7/10
8000/8000 [==============================] - 130s - loss: 2.1678 - acc: 0.4836 - val_loss: 2.6030 - val_acc: 0.3505
Epoch 8/10
8000/8000 [==============================] - 124s - loss: 2.1555 - acc: 0.4844 - val_loss: 2.6018 - val_acc: 0.3515
Epoch 9/10
8000/8000 [==============================] - 105s - loss: 2.1206 - acc: 0.4959 - val_loss: 2.7069 - val_acc: 0.3100
Epoch 10/10
8000/8000 [==============================] - 94s - loss: 2.1222 - acc: 0.5088 - val_loss: 2.5770 - val_acc: 0.3625
2000/2000 [==============================] - 10s    
2.62715686798 0.3555
[ 2  3  2 11  3  0  7  0  2  2  3  1  0  9  0  3  2  2 13  0 11  0 12  2  0
  0  3  2  7 10  6 10  9  9 11  6  2  3 13  3  7  1  3  5 11  9  0  3  9  9
  9  7  2 11  2  2  2  1  0  9  9  0 11  2  7 12  3  2 11  0  4  9  0  3 12
  2  4  0  5  9  9 12 13  3  2  1  0  3  9  0  3  0  2  2 10 13 11  7 12 11]
[ 2  3  3 12  3  0  0  0  0 11  0  3  0  9  0  3  2  3 11 13 11  0  2  2  0
  0  3  3  3  9  0  9 11  9  9  7  2  3  3  3  2  3  3  7  0 11  0  3 13  0
  3  9  2 11 11  3  1  9  3  7  9  3 11  9 11 11  3  0 11  0  7  9  0  3 12
  9  7  0  0  3  9 11 13  0  6  3  0  3 11  0 13  0  2  3  3  9  3 11 13  3]
[[257   0   0  44   0   0   1   4   9   0  12   5   2]
 [ 21   8   7  25   0   0   3  16   9   0   6   3   3]
 [ 35   4  53  99   0   0  11  12  23   3  38   5   5]
 [ 18   4  24 129   0   0   5   7  40   1  28   1   7]
 [  2   1   1   9   0   0   0   4   0   0   3   0   0]
 [  1   0   1   7   0   0   0   1   2   0   0   2   0]
 [  6   1   5  12   0   0   6   4   9   0   7   1   1]
 [ 22  10  12  34   0   0   1  31  24   2  37   5   4]
 [ 17   2   6  30   0   0   1   4  86   0  64  10   5]
 [  2   0   0   6   0   0   1   4  13   7  13   4   3]
 [ 28   2   2  21   0   0   1   3  32   3 103   6  11]
 [ 10   0   2  10   0   0   1   9  16   1  31  11   8]
 [ 19   1   0  17   0   0   0   3  30   1  49  16  20]]



#################

max

python conv_network.py 
Using TensorFlow backend.
['1856', '1865', '1875', '1883', '1897', '1903', '1911', '1924', '1936', '1945', '1957', '1965', '1972', '1983']
time: 2017-12-07 11:24:53.149808, epochs: 10, learning rate: 0.001, training size: 10000, test size: 2000
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
emb_layer (Embedding)        (None, 447, 300)          32738100  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 445, 128)          115328    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 222, 128)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 28416)             0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 28416)             113664    
_________________________________________________________________
dropout_1 (Dropout)          (None, 28416)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 14)                397838    
=================================================================
Total params: 33,364,930
Trainable params: 569,998
Non-trainable params: 32,794,932
_________________________________________________________________
None
Train on 8000 samples, validate on 2000 samples
Epoch 1/10
8000/8000 [==============================] - 117s - loss: 2.4326 - acc: 0.2679 - val_loss: 2.6625 - val_acc: 0.1515
Epoch 2/10
8000/8000 [==============================] - 130s - loss: 2.3499 - acc: 0.3565 - val_loss: 2.6170 - val_acc: 0.2295
Epoch 3/10
8000/8000 [==============================] - 131s - loss: 2.2753 - acc: 0.4137 - val_loss: 2.5369 - val_acc: 0.3420
Epoch 4/10
8000/8000 [==============================] - 128s - loss: 2.2311 - acc: 0.4490 - val_loss: 2.5856 - val_acc: 0.3510
Epoch 5/10
8000/8000 [==============================] - 129s - loss: 2.1767 - acc: 0.4731 - val_loss: 2.6140 - val_acc: 0.3505
Epoch 6/10
8000/8000 [==============================] - 127s - loss: 2.1426 - acc: 0.5020 - val_loss: 2.6193 - val_acc: 0.3670
Epoch 7/10
8000/8000 [==============================] - 127s - loss: 2.1428 - acc: 0.5084 - val_loss: 2.6374 - val_acc: 0.3525
Epoch 8/10
8000/8000 [==============================] - 130s - loss: 2.1411 - acc: 0.5160 - val_loss: 2.6743 - val_acc: 0.3615
Epoch 9/10
8000/8000 [==============================] - 129s - loss: 2.1062 - acc: 0.5356 - val_loss: 2.6414 - val_acc: 0.3805
Epoch 10/10
8000/8000 [==============================] - 117s - loss: 2.0978 - acc: 0.5404 - val_loss: 2.7070 - val_acc: 0.3625
2000/2000 [==============================] - 11s    
2.68635913086 0.3765
[11  0  0  3  2  7  7 12  9  0  0  7  0  6 11  3  0  1  2 11  3  0  7  2  9
  1  0  0 11  7 13 11  2  2  3  7  7 12  3  7 13  2 11 13  6  3 11  1  3 10
  0  3 11  1  7 12 13  0  2  1  7  0  2  0  9  2  0  0  2  0 12 10 11  3  7
  0  6  9  3  0  9  7  6 13 13 11  7 11  2  2  2  0 11 13  2 11  2 11 12  4]
[ 0  9  0  0  1 11  3  2 13  0  0  0  0  2 10  3  0 13  2  3  3  0  2  1  0
  0  0  0  3 10 13  2  2  2  3  2  3  0  3  7  0  0  0  7  7  0 10  0 13  0
  0  3 11  2  3  0  2  0  2  1  7  0  0  0  9  2  0  0  3 11 13  2 13  3  0
  0  2  0  3  0  9 12  0  7 13  0  0 12  2  2  4  0 12 13  7 13  2 11 13  7]
[[277   2  18  12   0   0   0   2   0   1   3   2   1   2]
 [ 28  18  31  12   3   0   0   6   0   1   0   2   0   6]
 [ 44   9 135  47   4   0   0  15   0   2   7   7   3  11]
 [ 30   4  63  85   0   0   1  17   0   4   9   3   3  21]
 [  0   1   4   3   0   0   1   2   0   1   0   0   0   1]
 [  4   0   0   6   0   0   0   2   0   1   1   0   2   3]
 [  3   4  11  17   0   0   4  10   0   1   0   0   1   3]
 [ 44   8  23  28   4   0   0  63   0   3   7   5  11   7]
 [  2   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [ 36   2  22  34   2   0   1  22   0  31   9  18   7  41]
 [  9   0   2   7   0   0   0   5   0   3  22   1   0   6]
 [ 41   3  15  24   0   0   0  11   0  16  18  49  11  32]
 [ 22   0   7  18   0   0   0  18   0  10   2   4  14  26]
 [ 29   2  11   9   1   0   0  10   0   9   5   2   4  55]]

