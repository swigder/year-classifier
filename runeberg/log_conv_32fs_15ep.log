python classifier.py conv
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
time: 2017-12-14 09:24:32.348930, epochs: 15, learning rate: 0.0003, training size: 100000, test size: 94875
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 293)               0         
_________________________________________________________________
emb_layer (Embedding)        (None, 293, 100)          2017200   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 293, 32)           9632      
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 146, 32)           0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4672)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 13)                60749     
=================================================================
Total params: 2,087,581
Trainable params: 2,087,581
Non-trainable params: 0
_________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/15
83200/83200 [==============================] - 104s - loss: 1.9559 - acc: 0.2376 - val_loss: 1.6493 - val_acc: 0.3180
Epoch 2/15
83200/83200 [==============================] - 102s - loss: 1.5151 - acc: 0.3699 - val_loss: 1.4962 - val_acc: 0.3771
Epoch 3/15
83200/83200 [==============================] - 101s - loss: 1.3788 - acc: 0.4225 - val_loss: 1.4645 - val_acc: 0.3875
Epoch 4/15
83200/83200 [==============================] - 103s - loss: 1.2917 - acc: 0.4607 - val_loss: 1.4412 - val_acc: 0.4068
Epoch 5/15
83200/83200 [==============================] - 102s - loss: 1.2155 - acc: 0.4958 - val_loss: 1.4437 - val_acc: 0.4142
Epoch 6/15
83200/83200 [==============================] - 102s - loss: 1.1461 - acc: 0.5279 - val_loss: 1.4469 - val_acc: 0.4217
Epoch 7/15
83200/83200 [==============================] - 103s - loss: 1.0814 - acc: 0.5608 - val_loss: 1.4629 - val_acc: 0.4251
Epoch 8/15
83200/83200 [==============================] - 102s - loss: 1.0228 - acc: 0.5876 - val_loss: 1.4948 - val_acc: 0.4286
Epoch 9/15
83200/83200 [==============================] - 103s - loss: 0.9661 - acc: 0.6162 - val_loss: 1.5275 - val_acc: 0.4319
Epoch 10/15
83200/83200 [==============================] - 104s - loss: 0.9116 - acc: 0.6410 - val_loss: 1.5627 - val_acc: 0.4296
Epoch 11/15
83200/83200 [==============================] - 103s - loss: 0.8565 - acc: 0.6666 - val_loss: 1.6075 - val_acc: 0.4308
Epoch 12/15
83200/83200 [==============================] - 104s - loss: 0.8004 - acc: 0.6928 - val_loss: 1.6716 - val_acc: 0.4300
Epoch 13/15
83200/83200 [==============================] - 103s - loss: 0.7457 - acc: 0.7183 - val_loss: 1.7374 - val_acc: 0.4276
Epoch 14/15
83200/83200 [==============================] - 102s - loss: 0.6916 - acc: 0.7432 - val_loss: 1.8023 - val_acc: 0.4289
Epoch 15/15
83200/83200 [==============================] - 104s - loss: 0.6374 - acc: 0.7680 - val_loss: 1.8829 - val_acc: 0.4291
94875/94875 [==============================] - 34s     
1.93501754802 0.410866930172
[11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11
 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]
[10 11 11 11 10 12 11 11 11 11 11 10 10 11 12 10 11 11 11 10 12 11 11 11 11
  8 12 11 11 11 10 11 10 11 12  9 12 11 11 11 11  7 10  6 10 11 11 10 10 11
 12 11 10  8  3 10 11  8 11 10 11 11 11 12 10 11 10 12 11 10 12 12 11 10 11
 10 11 10  8 10 11  9 12 10 12 10 12 11 10 11  9  1 10 10  4  9 11 11  9 11]
[[2006  967  381  110   44   25    8   12    7    1    2    5    2]
 [1687 3670 1843  344  133  161   50   47   39    1    9   14    2]
 [ 602 1657 3209 1325  324  477  181  111   65    4    4   31   10]
 [ 250  310 1363 3167 1551  674  362  213   59   10    9   20   12]
 [ 103  150  356 1798 3091 1213  706  374  113   30   27   30    9]
 [  76  221  551  853 1318 2425 1521  685  207   39   45   39   20]
 [  35   72  236  419  625 1315 2722 1796  421   81  159   92   27]
 [  32   66  133  208  378  462 1735 3190  968  215  334  222   57]
 [  24   25   65   60  125  151  313 1133 3371 1518  690  456   69]
 [   9    0    3    3   10   13   38  109  735 1788  307  227   63]
 [   9    4   18   13   37   22  169  305  621  647 2898 2460  797]
 [  17    8   17   23   71   11   70  189  369  412 1810 3392 1611]
 [   6    0   15    7   42    5   33   95  152  203  928 2462 4052]]
             precision    recall  f1-score   support

          0       0.41      0.56      0.48      3570
          1       0.51      0.46      0.48      8000
          2       0.39      0.40      0.40      8000
          3       0.38      0.40      0.39      8000
          4       0.40      0.39      0.39      8000
          5       0.35      0.30      0.32      8000
          6       0.34      0.34      0.34      8000
          7       0.39      0.40      0.39      8000
          8       0.47      0.42      0.45      8000
          9       0.36      0.54      0.43      3305
         10       0.40      0.36      0.38      8000
         11       0.36      0.42      0.39      8000
         12       0.60      0.51      0.55      8000

avg / total       0.42      0.41      0.41     94875


