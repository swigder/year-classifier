python classifier.py conv
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
time: 2017-12-13 13:35:25.385343, epochs: 8, learning rate: 0.0003, training size: 100000, test size: 94875
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 293)               0         
_________________________________________________________________
emb_layer (Embedding)        (None, 293, 100)          2017200   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 293, 32)           9632      
_________________________________________________________________
batch_normalization_1 (Batch (None, 293, 32)           128       
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 146, 32)           0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4672)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 13)                60749     
=================================================================
Total params: 2,087,709
Trainable params: 2,087,645
Non-trainable params: 64
_________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/8
83200/83200 [==============================] - 206s - loss: 1.6948 - acc: 0.3229 - val_loss: 1.5581 - val_acc: 0.3571
Epoch 2/8
83200/83200 [==============================] - 213s - loss: 1.2398 - acc: 0.4952 - val_loss: 1.4487 - val_acc: 0.4152
Epoch 3/8
83200/83200 [==============================] - 202s - loss: 0.9972 - acc: 0.6103 - val_loss: 1.7034 - val_acc: 0.3992
Epoch 4/8
83200/83200 [==============================] - 200s - loss: 0.7689 - acc: 0.7127 - val_loss: 1.8244 - val_acc: 0.4058
Epoch 5/8
83200/83200 [==============================] - 192s - loss: 0.5309 - acc: 0.8201 - val_loss: 1.9533 - val_acc: 0.4312
Epoch 6/8
83200/83200 [==============================] - 165s - loss: 0.3210 - acc: 0.9037 - val_loss: 2.2989 - val_acc: 0.4158
Epoch 7/8
83200/83200 [==============================] - 158s - loss: 0.1806 - acc: 0.9516 - val_loss: 2.8686 - val_acc: 0.4027
Epoch 8/8
83200/83200 [==============================] - 158s - loss: 0.1032 - acc: 0.9744 - val_loss: 3.0915 - val_acc: 0.4151
94848/94875 [============================>.] - ETA: 0s3.17022377005 0.395625823448
[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]
[ 2  2  2  3  2  1  5  2  3  5  6  2  3  3  4  3  8  2  3  2  2  1  3  2  0
  5  6  3  3  3  4  3  2  4  2  3  2  4  6  2  5  3  3  2  5  2  5  1  4  2
  3  3  3  3  4  3  4  5  2  6  6  6  6  6  2  3  2  4  3  2  2  2  3  2 10
  2  3  8  3  3  3  3  3  3  3  3  6  3  5  2  4  3  4  5  6  3  0  2  2  3]
[[1953  796  684   66   13   18   26    4    5    2    1    2    0]
 [1326 3283 2881  226   44   87   92   10   27   13    3    7    1]
 [ 446 1369 4462  885  164  285  269   29   48   26    8    5    4]
 [ 156  316 2227 2903 1036  583  547   63   83   43   30   10    3]
 [  77  153  953 1883 2308 1193 1064  149  104   61   28   22    5]
 [  85  200 1006  865  900 2181 2091  278  203   98   55   24   14]
 [  28   95  474  371  450 1206 3670  901  412  152  147   60   34]
 [  28   65  403  215  227  484 2788 1707 1177  377  314  150   65]
 [  14   27  182   70   34  114  609  516 3409 2172  544  231   78]
 [   0    2   18    5    7   11   47   34  596 2143  233  154   55]
 [   1    7   67   18   10   31  276  145  688 1087 3026 1781  863]
 [   1    7  126   13   10   30  229   81  407  701 2359 2461 1575]
 [   0    3   78    4    4   16  155   56  210  308 1404 1733 4029]]
             precision    recall  f1-score   support

          0       0.47      0.55      0.51      3570
          1       0.52      0.41      0.46      8000
          2       0.33      0.56      0.41      8000
          3       0.39      0.36      0.37      8000
          4       0.44      0.29      0.35      8000
          5       0.35      0.27      0.31      8000
          6       0.31      0.46      0.37      8000
          7       0.43      0.21      0.29      8000
          8       0.46      0.43      0.44      8000
          9       0.30      0.65      0.41      3305
         10       0.37      0.38      0.37      8000
         11       0.37      0.31      0.34      8000
         12       0.60      0.50      0.55      8000

avg / total       0.41      0.40      0.39     94875

