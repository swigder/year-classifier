 python classifier.py conv
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
time: 2017-12-13 11:41:11.893220, epochs: 15, learning rate: 0.0001, training size: 100000, test size: 94875
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 293)               0         
_________________________________________________________________
emb_layer (Embedding)        (None, 293, 100)          2017200   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 293, 64)           19264     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 146, 64)           0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 9344)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 13)                121485    
=================================================================
Total params: 2,157,949
Trainable params: 2,157,949
Non-trainable params: 0
_________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/15
83200/83200 [==============================] - 252s - loss: 2.1697 - acc: 0.1881 - val_loss: 1.8453 - val_acc: 0.2618
Epoch 2/15
83200/83200 [==============================] - 245s - loss: 1.7530 - acc: 0.2811 - val_loss: 1.7180 - val_acc: 0.2864
Epoch 3/15
83200/83200 [==============================] - 246s - loss: 1.6371 - acc: 0.3211 - val_loss: 1.6508 - val_acc: 0.3104
Epoch 4/15
83200/83200 [==============================] - 252s - loss: 1.5428 - acc: 0.3617 - val_loss: 1.5830 - val_acc: 0.3433
Epoch 5/15
83200/83200 [==============================] - 251s - loss: 1.4549 - acc: 0.3979 - val_loss: 1.5342 - val_acc: 0.3665
Epoch 6/15
83200/83200 [==============================] - 252s - loss: 1.3868 - acc: 0.4249 - val_loss: 1.5049 - val_acc: 0.3787
Epoch 7/15
83200/83200 [==============================] - 254s - loss: 1.3323 - acc: 0.4489 - val_loss: 1.4868 - val_acc: 0.3863
Epoch 8/15
83200/83200 [==============================] - 255s - loss: 1.2854 - acc: 0.4689 - val_loss: 1.4760 - val_acc: 0.3972
Epoch 9/15
83200/83200 [==============================] - 254s - loss: 1.2428 - acc: 0.4888 - val_loss: 1.4656 - val_acc: 0.4030
Epoch 10/15
83200/83200 [==============================] - 249s - loss: 1.2018 - acc: 0.5081 - val_loss: 1.4588 - val_acc: 0.4119
Epoch 11/15
83200/83200 [==============================] - 257s - loss: 1.1617 - acc: 0.5264 - val_loss: 1.4532 - val_acc: 0.4196
Epoch 12/15
83200/83200 [==============================] - 253s - loss: 1.1232 - acc: 0.5434 - val_loss: 1.4520 - val_acc: 0.4252
Epoch 13/15
83200/83200 [==============================] - 250s - loss: 1.0860 - acc: 0.5621 - val_loss: 1.4553 - val_acc: 0.4294
Epoch 14/15
83200/83200 [==============================] - 252s - loss: 1.0512 - acc: 0.5778 - val_loss: 1.4643 - val_acc: 0.4292
Epoch 15/15
83200/83200 [==============================] - 253s - loss: 1.0180 - acc: 0.5941 - val_loss: 1.4708 - val_acc: 0.4340
94848/94875 [============================>.] - ETA: 0s1.51852930731 0.405849802374
[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]
[11  3  2  3  3  2  5  3  3  3  5  2  3  3  6  4  7  3  3  2  3  0  3  5  4
  4  6  3  3  3  4  3 11  6  2  3  2  3  4  3  1  2  5  3  5  5  3  1  4  3
  4  3  3  3  3  3  4  7  3  3  4  4  5  6  3  3  2  5  2  4  1  2  3  3  6
  3  5  7  3  3  3  3  3  3  3  3  3  3  3  0  4  3  4  6  3  4  0  2  2  2]
[[2433  709  257   90   24   27    4    5    3    0    0   18    0]
 [2756 2733 1793  328  101  170   34   35   10    1    3   36    0]
 [ 966 1249 3198 1494  235  538  180   74   18    2    2   41    3]
 [ 359  177 1309 3363 1464  689  392  126   27   12    9   71    2]
 [ 160   92  288 1994 3099 1099  784  283   67   34   17   80    3]
 [ 133  132  560 1019 1299 2190 1700  662  126   41   45   78   15]
 [  44   39  193  395  614 1323 2984 1729  265   98  168  131   17]
 [  47   20  112  171  325  574 2120 2680 1006  226  374  304   41]
 [  32   13   35   58  114  198  293  774 3643 1781  616  410   33]
 [   7    0    1    6   16    4   21   78  709 1934  247  243   39]
 [   5    1   10   11   15   10  108  295  643  815 2599 2429 1059]
 [  13    3   11   16    7    5   77  169  303  474 1703 2964 2255]
 [   2    2   12    5    0    1   29   70  125  179  858 2032 4685]]
             precision    recall  f1-score   support

          0       0.35      0.68      0.46      3570
          1       0.53      0.34      0.42      8000
          2       0.41      0.40      0.41      8000
          3       0.38      0.42      0.40      8000
          4       0.42      0.39      0.40      8000
          5       0.32      0.27      0.30      8000
          6       0.34      0.37      0.36      8000
          7       0.38      0.34      0.36      8000
          8       0.52      0.46      0.49      8000
          9       0.35      0.59      0.43      3305
         10       0.39      0.32      0.36      8000
         11       0.34      0.37      0.35      8000
         12       0.57      0.59      0.58      8000

avg / total       0.41      0.41      0.40     94875

