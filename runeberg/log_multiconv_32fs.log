python classifier.py multiconv
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
time: 2017-12-13 11:18:31.387555, epochs: 15, learning rate: 0.0001, training size: 100000, test size: 94875
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 293)           0                                            
____________________________________________________________________________________________________
emb_layer (Embedding)            (None, 293, 100)      2017200     input_1[0][0]                    
____________________________________________________________________________________________________
conv1d_1 (Conv1D)                (None, 293, 32)       9632        emb_layer[0][0]                  
____________________________________________________________________________________________________
conv1d_2 (Conv1D)                (None, 293, 32)       16032       emb_layer[0][0]                  
____________________________________________________________________________________________________
conv1d_3 (Conv1D)                (None, 293, 32)       22432       emb_layer[0][0]                  
____________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)   (None, 293, 100)      0           emb_layer[0][0]                  
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 293, 196)      0           conv1d_1[0][0]                   
                                                                   conv1d_2[0][0]                   
                                                                   conv1d_3[0][0]                   
                                                                   max_pooling1d_1[0][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 57428)         0           concatenate_1[0][0]              
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 13)            746577      flatten_1[0][0]                  
====================================================================================================
Total params: 2,811,873
Trainable params: 2,811,873
Non-trainable params: 0
____________________________________________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/15
83200/83200 [==============================] - 311s - loss: 2.0040 - acc: 0.2324 - val_loss: 1.6705 - val_acc: 0.3088
Epoch 2/15
83200/83200 [==============================] - 305s - loss: 1.5439 - acc: 0.3612 - val_loss: 1.5309 - val_acc: 0.3676
Epoch 3/15
83200/83200 [==============================] - 330s - loss: 1.4007 - acc: 0.4175 - val_loss: 1.4723 - val_acc: 0.3886
Epoch 4/15
83200/83200 [==============================] - 329s - loss: 1.3056 - acc: 0.4588 - val_loss: 1.4428 - val_acc: 0.4053
Epoch 5/15
83200/83200 [==============================] - 360s - loss: 1.2309 - acc: 0.4937 - val_loss: 1.4239 - val_acc: 0.4191
Epoch 6/15
83200/83200 [==============================] - 373s - loss: 1.1639 - acc: 0.5249 - val_loss: 1.4162 - val_acc: 0.4309
Epoch 7/15
83200/83200 [==============================] - 375s - loss: 1.1015 - acc: 0.5549 - val_loss: 1.4201 - val_acc: 0.4363
Epoch 8/15
83200/83200 [==============================] - 390s - loss: 1.0436 - acc: 0.5839 - val_loss: 1.4216 - val_acc: 0.4476
Epoch 9/15
83200/83200 [==============================] - 379s - loss: 0.9882 - acc: 0.6095 - val_loss: 1.4393 - val_acc: 0.4470
Epoch 10/15
83200/83200 [==============================] - 390s - loss: 0.9365 - acc: 0.6329 - val_loss: 1.4567 - val_acc: 0.4488
Epoch 11/15
83200/83200 [==============================] - 378s - loss: 0.8870 - acc: 0.6571 - val_loss: 1.4766 - val_acc: 0.4537
Epoch 12/15
83200/83200 [==============================] - 390s - loss: 0.8398 - acc: 0.6801 - val_loss: 1.5075 - val_acc: 0.4555
Epoch 13/15
83200/83200 [==============================] - 375s - loss: 0.7947 - acc: 0.7012 - val_loss: 1.5353 - val_acc: 0.4554
Epoch 14/15
83200/83200 [==============================] - 385s - loss: 0.7503 - acc: 0.7215 - val_loss: 1.5724 - val_acc: 0.4554
Epoch 15/15
83200/83200 [==============================] - 345s - loss: 0.7081 - acc: 0.7415 - val_loss: 1.6175 - val_acc: 0.4506
94848/94875 [============================>.] - ETA: 0s1.63039376648 0.436416337289
[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]
[7 2 2 2 3 2 5 4 3 3 6 3 4 3 4 4 6 2 3 2 2 2 3 3 4 4 6 4 3 4 4 4 4 6 2 3 2
 4 4 2 5 2 3 2 5 4 3 0 4 3 5 4 3 4 4 3 4 6 3 6 6 3 6 6 2 3 2 4 2 4 2 2 3 3
 5 3 5 9 3 2 3 6 4 2 3 4 4 3 3 2 3 3 3 5 5 3 0 2 2 3]
[[2225  794  350   87   42   29   12   17    7    0    1    6    0]
 [1584 3851 1910  186  163  142   70   35   26    2   10   18    3]
 [ 555 1540 3817  929  393  380  211   81   56    8    8   18    4]
 [ 232  259 1688 2728 1835  569  395  183   55   15   14   22    5]
 [  99  121  439 1313 3663  977  877  331   92   26   12   42    8]
 [  91  167  590  577 1446 2269 2046  493  157   37   48   55   24]
 [  34   62  242  249  675 1018 3523 1492  347   77  127  112   42]
 [  33   38  169  145  396  213 2146 2979 1036  255  352  179   59]
 [  22   16   73   45  110   94  373  897 3659 1705  655  295   56]
 [   2    2   12    2   11    7   32   80  658 1999  244  204   52]
 [   5    5   26   14   23   14  155  269  516  745 3174 2360  694]
 [   3    6   19   12   29   10  112  229  252  407 2070 3329 1522]
 [   1    1   16    8   12    9   68  100  103  187  998 2308 4189]]
             precision    recall  f1-score   support

          0       0.46      0.62      0.53      3570
          1       0.56      0.48      0.52      8000
          2       0.41      0.48      0.44      8000
          3       0.43      0.34      0.38      8000
          4       0.42      0.46      0.44      8000
          5       0.40      0.28      0.33      8000
          6       0.35      0.44      0.39      8000
          7       0.41      0.37      0.39      8000
          8       0.53      0.46      0.49      8000
          9       0.37      0.60      0.46      3305
         10       0.41      0.40      0.40      8000
         11       0.37      0.42      0.39      8000
         12       0.63      0.52      0.57      8000

avg / total       0.44      0.44      0.44     94875

