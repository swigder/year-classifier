 python classifier.py lstm
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
time: 2017-12-13 13:16:07.352785, epochs: 5, learning rate: 0.00025, training size: 100000, test size: 94875
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, None, 100)         2017300   
_________________________________________________________________
masking_1 (Masking)          (None, None, 100)         0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, 20)                8880      
_________________________________________________________________
batch_normalization_1 (Batch (None, 20)                80        
_________________________________________________________________
dense_1 (Dense)              (None, 13)                273       
=================================================================
Total params: 2,026,533
Trainable params: 2,026,493
Non-trainable params: 40
_________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/5
83200/83200 [==============================] - 586s - loss: 2.1175 - acc: 0.2446 - val_loss: 1.8711 - val_acc: 0.3344
Epoch 2/5
83200/83200 [==============================] - 634s - loss: 1.5010 - acc: 0.4454 - val_loss: 1.6403 - val_acc: 0.3760
Epoch 3/5
83200/83200 [==============================] - 696s - loss: 1.2575 - acc: 0.5305 - val_loss: 1.5913 - val_acc: 0.3837
Epoch 4/5
83200/83200 [==============================] - 584s - loss: 1.1141 - acc: 0.5856 - val_loss: 2.1060 - val_acc: 0.2853
Epoch 5/5
83200/83200 [==============================] - 498s - loss: 1.0093 - acc: 0.6236 - val_loss: 1.6268 - val_acc: 0.3802
94875/94875 [==============================] - 170s     
1.67390813349 0.359009222663
[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]
[ 2  2  2  5  3  2  6  3  2  5  4  4  4  4  4  4  5  2  5  2  2  1  2  2  6
  3  3  2  4  3  4  4  2  7  2  3  2  2  2  2  5  1  4  3  2  2  4 11  4  3
  4  3  4  4  3  3  4  7  4  5  4  3  6  2  2  2  2  2  2  2  2  1  4  1  3
  2  1  4  4  5  4  5  3  4  3  2  6  3  4  2  2  5  3  5  4  4  1  2  2  1]
[[1437 1692  272   11   47   44   27   21    7    2    0    6    4]
 [1245 5275  941   41  211  165   30   35   21   10    0   17    9]
 [ 446 3006 2445  512  766  544  125   70   48   13    2   13   10]
 [  99  734 1677 1586 2318 1065  312  116   41   18    3   17   14]
 [  98  255  707 1170 3095 1715  695  169   33   10    1   33   19]
 [ 106  312  607  659 1400 2937 1332  440   59   34   13   55   46]
 [  75  122  263  511  670 2145 2533 1236  129   68   15  136   97]
 [ 118   99  318  442  332 1047 2221 2325  360  263   30  312  133]
 [ 130   97  227  244   67  310  407 2327 1186 2194   67  641  103]
 [  26   21   46   78    1   37   22  297  177 2074   51  412   63]
 [  28   39   51  151   16  185  274  621   43 1015  362 3978 1237]
 [  24   33  112  116   12  103  221  368   19  524  194 4281 1993]
 [  12   15   66   44    6   66  139  184    4  230   80 2629 4525]]
             precision    recall  f1-score   support

          0       0.37      0.40      0.39      3570
          1       0.45      0.66      0.54      8000
          2       0.32      0.31      0.31      8000
          3       0.28      0.20      0.23      8000
          4       0.35      0.39      0.37      8000
          5       0.28      0.37      0.32      8000
          6       0.30      0.32      0.31      8000
          7       0.28      0.29      0.29      8000
          8       0.56      0.15      0.23      8000
          9       0.32      0.63      0.43      3305
         10       0.44      0.05      0.08      8000
         11       0.34      0.54      0.42      8000
         12       0.55      0.57      0.56      8000

avg / total       0.38      0.36      0.34     94875


