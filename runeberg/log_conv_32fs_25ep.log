python classifier.py conv
Using TensorFlow backend.
len 104000
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
len 94875
['1760', '1780', '1800', '1820', '1840', '1860', '1880', '1900', '1920', '1940', '1960', '1980', '2000']
vocab = 20172
(104000, 293)
time: 2017-12-13 12:58:12.092701, epochs: 25, learning rate: 0.00025, training size: 100000, test size: 94875
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 293)               0         
_________________________________________________________________
emb_layer (Embedding)        (None, 293, 100)          2017200   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 293, 32)           9632      
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 146, 32)           0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4672)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 13)                60749     
=================================================================
Total params: 2,087,581
Trainable params: 2,087,581
Non-trainable params: 0
_________________________________________________________________
None
Train on 83200 samples, validate on 20800 samples
Epoch 1/25
83200/83200 [==============================] - 123s - loss: 1.9903 - acc: 0.2265 - val_loss: 1.7092 - val_acc: 0.2929
Epoch 2/25
83200/83200 [==============================] - 125s - loss: 1.5875 - acc: 0.3400 - val_loss: 1.5511 - val_acc: 0.3562
Epoch 3/25
83200/83200 [==============================] - 126s - loss: 1.4098 - acc: 0.4116 - val_loss: 1.4711 - val_acc: 0.3886
Epoch 4/25
83200/83200 [==============================] - 124s - loss: 1.2974 - acc: 0.4598 - val_loss: 1.4489 - val_acc: 0.4067
Epoch 5/25
83200/83200 [==============================] - 120s - loss: 1.2186 - acc: 0.4948 - val_loss: 1.4484 - val_acc: 0.4124
Epoch 6/25
83200/83200 [==============================] - 120s - loss: 1.1534 - acc: 0.5259 - val_loss: 1.4585 - val_acc: 0.4168
Epoch 7/25
83200/83200 [==============================] - 126s - loss: 1.0938 - acc: 0.5557 - val_loss: 1.4705 - val_acc: 0.4233
Epoch 8/25
83200/83200 [==============================] - 125s - loss: 1.0391 - acc: 0.5834 - val_loss: 1.4919 - val_acc: 0.4298
Epoch 9/25
83200/83200 [==============================] - 120s - loss: 0.9866 - acc: 0.6067 - val_loss: 1.5223 - val_acc: 0.4315
Epoch 10/25
83200/83200 [==============================] - 124s - loss: 0.9350 - acc: 0.6313 - val_loss: 1.5476 - val_acc: 0.4354
Epoch 11/25
83200/83200 [==============================] - 122s - loss: 0.8836 - acc: 0.6552 - val_loss: 1.5787 - val_acc: 0.4370
Epoch 12/25
83200/83200 [==============================] - 121s - loss: 0.8332 - acc: 0.6790 - val_loss: 1.6255 - val_acc: 0.4337
Epoch 13/25
83200/83200 [==============================] - 120s - loss: 0.7826 - acc: 0.7016 - val_loss: 1.6746 - val_acc: 0.4399
Epoch 14/25
83200/83200 [==============================] - 118s - loss: 0.7345 - acc: 0.7234 - val_loss: 1.7325 - val_acc: 0.4354
Epoch 15/25
83200/83200 [==============================] - 125s - loss: 0.6876 - acc: 0.7457 - val_loss: 1.8034 - val_acc: 0.4307
Epoch 16/25
83200/83200 [==============================] - 124s - loss: 0.6417 - acc: 0.7672 - val_loss: 1.8812 - val_acc: 0.4278
Epoch 17/25
83200/83200 [==============================] - 136s - loss: 0.5969 - acc: 0.7875 - val_loss: 1.9610 - val_acc: 0.4307
Epoch 18/25
83200/83200 [==============================] - 138s - loss: 0.5516 - acc: 0.8074 - val_loss: 2.0472 - val_acc: 0.4266
Epoch 19/25
83200/83200 [==============================] - 145s - loss: 0.5087 - acc: 0.8258 - val_loss: 2.1570 - val_acc: 0.4242
Epoch 20/25
83200/83200 [==============================] - 147s - loss: 0.4674 - acc: 0.8438 - val_loss: 2.2529 - val_acc: 0.4226
Epoch 21/25
83200/83200 [==============================] - 143s - loss: 0.4253 - acc: 0.8624 - val_loss: 2.3616 - val_acc: 0.4195
Epoch 22/25
83200/83200 [==============================] - 140s - loss: 0.3867 - acc: 0.8797 - val_loss: 2.4867 - val_acc: 0.4201
Epoch 23/25
83200/83200 [==============================] - 138s - loss: 0.3490 - acc: 0.8941 - val_loss: 2.6048 - val_acc: 0.4195
Epoch 24/25
83200/83200 [==============================] - 135s - loss: 0.3129 - acc: 0.9080 - val_loss: 2.7386 - val_acc: 0.4163
Epoch 25/25
83200/83200 [==============================] - 132s - loss: 0.2789 - acc: 0.9218 - val_loss: 2.8764 - val_acc: 0.4124
94848/94875 [============================>.] - ETA: 0s2.85243387025 0.398988142289
[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]
[7 4 2 5 3 3 7 0 3 5 6 2 3 3 4 4 7 2 4 2 3 0 3 4 4 7 5 3 3 4 4 4 7 4 3 3 2
 4 1 3 6 3 4 3 2 2 2 7 3 3 5 3 3 5 3 3 3 4 3 6 6 4 6 3 3 3 2 3 2 2 8 2 3 1
 6 0 5 7 4 2 3 3 3 5 3 5 1 3 3 3 3 4 4 5 5 5 0 2 2 2]
[[2098  856  337  127   44   50    8   27   12    3    2    3    3]
 [1754 3682 1588  413  182  193   59   53   50    3    9    9    5]
 [ 684 1607 2882 1467  429  527  178  110   78    7    9   14    8]
 [ 280  329 1184 3129 1633  739  359  223   66   12   17   18   11]
 [ 123  128  317 1722 3097 1289  742  389  103   23   20   25   22]
 [ 128  198  444  847 1417 2525 1519  555  195   50   39   57   26]
 [  35   71  187  392  778 1464 2778 1512  432   74  125  103   49]
 [  60   46  150  251  398  550 1879 2750 1030  245  330  220   91]
 [  32   21   48   93  101  175  361 1033 3472 1560  596  371  137]
 [   2    3    4   12    7   29   46  112  756 1714  279  237  104]
 [   9    4   27   22   32   33  159  393  708  681 2452 2452 1028]
 [   5    5   18   25   28   24  113  249  423  379 1719 3112 1900]
 [   0    3   14    8   11   22   64  131  204  171  952 2257 4163]]
             precision    recall  f1-score   support

          0       0.40      0.59      0.48      3570
          1       0.53      0.46      0.49      8000
          2       0.40      0.36      0.38      8000
          3       0.37      0.39      0.38      8000
          4       0.38      0.39      0.38      8000
          5       0.33      0.32      0.32      8000
          6       0.34      0.35      0.34      8000
          7       0.36      0.34      0.35      8000
          8       0.46      0.43      0.45      8000
          9       0.35      0.52      0.42      3305
         10       0.37      0.31      0.34      8000
         11       0.35      0.39      0.37      8000
         12       0.55      0.52      0.54      8000

avg / total       0.40      0.40      0.40     94875

